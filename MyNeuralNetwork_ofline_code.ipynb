{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "ANemiyD7YFPr"
      },
      "outputs": [],
      "source": [
        "## Importing needed libraries\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.utils import shuffle"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "seL5nROFYJZZ",
        "outputId": "5c89f6a4-b8e5-4e8f-bd11-1c61588c0c33"
      },
      "outputs": [],
      "source": [
        "## Mounting Google Drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "GXsv3YAfmj3h",
        "outputId": "78cc52f8-7fd8-44d1-be7c-381d33cc9562"
      },
      "outputs": [],
      "source": [
        "np.random.seed(42)\n",
        "## Building class named Neural_Network\n",
        "class Neural_Network:\n",
        "  def __init__(self,Input_layer_size,Output_layer_size,Hidden_layer_size):\n",
        "    self.Input_layer_size=Input_layer_size\n",
        "    self.Hidden_layer_size=Hidden_layer_size\n",
        "    self.Output_layer_size=Output_layer_size\n",
        "\n",
        "    ## Weights and Bias Intialization\n",
        "    self.Weights_I_H=np.random.randn(self.Input_layer_size,self.Hidden_layer_size) #Initializing weights for input to hidden layer of network\n",
        "    self.Bias_H=np.zeros((1,self.Hidden_layer_size)) #Initializing bias to hidden layer\n",
        "    self.Weights_H_O=np.random.randn(self.Hidden_layer_size,self.Output_layer_size) #Initializing weights for hidden to output layer\n",
        "    self.Bias_O=np.zeros((1,self.Output_layer_size)) # Initializing bias to output layer\n",
        "    self.DeltaWeight_I_H=np.zeros((self.Input_layer_size,self.Hidden_layer_size)) #Delta weights\n",
        "    self.DeltaWeights_H_O=np.zeros((self.Hidden_layer_size,self.Output_layer_size)) #Delta weights\n",
        "\n",
        "  ## defining Sigmoid activation function\n",
        "  def sigmoid(self,x,lmda=0.1):\n",
        "    return 1 / (1 + np.exp(-lmda * x))\n",
        "\n",
        "  ## defining derivative of activation function\n",
        "  def sigmoid_derivative(self,x,lmda=0.1):\n",
        "    return ((lmda * x) * (1 - x))\n",
        "\n",
        "  ## defining normalization function\n",
        "  def Normalize_data(self,data):\n",
        "    Min_value=np.min(data,axis=0)\n",
        "    Max_value=np.max(data,axis=0)\n",
        "    return (data-Min_value)/(Max_value-Min_value)\n",
        "\n",
        "  ## defining function for spliting data\n",
        "  def data_split(self,data,train_ratio=0.7, Val_ratio=0.15):\n",
        "\n",
        "    Train_size=int(len(data) * train_ratio)\n",
        "    Validation_size=int(len(data) * Val_ratio)\n",
        "\n",
        "    Train_data = data[:Train_size]\n",
        "    Validation_data = data[Train_size:(Train_size + Validation_size)]\n",
        "    Test_data = data[(Train_size+Validation_size):]\n",
        "    return Train_data , Validation_data , Test_data\n",
        "\n",
        "  def prediction(self,x):\n",
        "    hidden_layer_output=self.sigmoid(np.dot(x,self.Weights_I_H)+self.Bias_H)\n",
        "    predicted_output=self.sigmoid(np.dot(hidden_layer_output,self.Weights_H_O)+self.Bias_O)\n",
        "    return predicted_output\n",
        "\n",
        "  # Feed Forward\n",
        "  def Feed_Forward(self,x):\n",
        "    self.Hidden=self.sigmoid(np.dot(x,self.Weights_I_H)+self.Bias_H)\n",
        "    self.Output=self.sigmoid(np.dot(self.Hidden,self.Weights_H_O)+self.Bias_O)\n",
        "    return self.Output\n",
        "\n",
        "  # Back Propagation\n",
        "  def Back_Propagation(self, x, y, learning_rate, momentum):\n",
        "    ## Calculating Output Error\n",
        "    Output_Error = y - self.Output\n",
        "    Delta_Output_layer = Output_Error * self.sigmoid_derivative(self.Output)\n",
        "\n",
        "    ## Calculating Error at Hidden\n",
        "    Hidden_Error = Delta_Output_layer.dot(self.Weights_H_O.T)\n",
        "    Delta_Hidden_layer = Hidden_Error * self.sigmoid_derivative(self.Hidden)\n",
        "\n",
        "    ## Updating Weights and Bias\n",
        "    self.Weights_H_O += (self.Hidden.T.dot(Delta_Output_layer) * learning_rate) + (momentum * self.DeltaWeights_H_O)\n",
        "    self.DeltaWeights_H_O = (self.Hidden.T.dot(Delta_Output_layer) * learning_rate) + (momentum * self.DeltaWeights_H_O)\n",
        "    self.Bias_O += np.sum(Delta_Output_layer, axis=0) * learning_rate\n",
        "    self.Weights_I_H += (x.T.dot(Delta_Hidden_layer) * learning_rate) + (momentum * self.DeltaWeight_I_H)\n",
        "    self.DeltaWeight_I_H = (x.T.dot(Delta_Hidden_layer) * learning_rate) + (momentum * self.DeltaWeight_I_H)\n",
        "    self.Bias_H += np.sum(Delta_Hidden_layer, axis=0) * learning_rate\n",
        "\n",
        "  def Training(self, x_Train, y_Train, x_Validation, y_Validation, x_Test, y_Test, learning_rate, momentum, epochs, early_stop_patience=5):\n",
        "    Train_Losses = []\n",
        "    Validation_Losses = []\n",
        "    Test_Losses = []\n",
        "\n",
        "    Best_Validation_Loss = float('inf')\n",
        "    epochs_without_imp = 0\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        epoch_train_loss = 0\n",
        "\n",
        "        for i in range(len(x_Train)):\n",
        "            x_row = x_Train[i].reshape(1, -1)\n",
        "            y_row = y_Train[i].reshape(1, -1)\n",
        "\n",
        "            self.Feed_Forward(x_row)\n",
        "            self.Back_Propagation(x_row, y_row, learning_rate, momentum)\n",
        "\n",
        "            # Calculate and store training loss for the current row\n",
        "            epoch_train_loss += np.sqrt(np.mean(np.square(y_row - self.Feed_Forward(x_row))))\n",
        "\n",
        "        # Calculate average training loss for the epoch\n",
        "        avg_train_loss = epoch_train_loss / len(x_Train)\n",
        "        Train_Losses.append(avg_train_loss)\n",
        "\n",
        "        # Calculate and store validation loss for the entire validation set\n",
        "        validation_loss = 0\n",
        "        for i in range(len(x_Validation)):\n",
        "            x_row_validation = x_Validation[i].reshape(1, -1)\n",
        "            y_row_validation = y_Validation[i].reshape(1, -1)\n",
        "            validation_loss += np.sqrt(np.mean(np.square(y_row_validation - self.Feed_Forward(x_row_validation))))\n",
        "\n",
        "        avg_validation_loss = validation_loss / len(x_Validation)\n",
        "        Validation_Losses.append(avg_validation_loss)\n",
        "\n",
        "        # Calculate and store test loss for the entire test set\n",
        "        test_loss = 0\n",
        "        for i in range(len(x_Test)):\n",
        "            x_row_Test = x_Test[i].reshape(1, -1)\n",
        "            y_row_Test = y_Test[i].reshape(1, -1)\n",
        "            test_loss += np.sqrt(np.mean(np.square(y_row_Test - self.Feed_Forward(x_row_Test))))\n",
        "\n",
        "        avg_test_loss = test_loss / len(x_Test)\n",
        "        Test_Losses.append(avg_test_loss)\n",
        "\n",
        "        print(\"\\nEpoch:\", epoch, \"Training Loss:\", avg_train_loss, \"Validation Loss:\", avg_validation_loss, \"Test Loss:\", avg_test_loss)\n",
        "\n",
        "        if avg_validation_loss < Best_Validation_Loss:\n",
        "            Best_Validation_Loss = avg_validation_loss\n",
        "            epochs_without_imp = 0\n",
        "        else:\n",
        "            epochs_without_imp += 1\n",
        "\n",
        "        if epochs_without_imp >= early_stop_patience:\n",
        "            print(f'Early stopping at epoch {epoch} owing to no improvement for {early_stop_patience} epochs.')\n",
        "            break\n",
        "\n",
        "    self.plot_losses(Train_Losses, Validation_Losses)\n",
        "    return Train_Losses, Validation_Losses, Test_Losses\n",
        "\n",
        "\n",
        "  def plot_losses(self,Train_Losses,Validation_Losses):\n",
        "    epochs=range(0 , len(Train_Losses))\n",
        "    plt.figure(figsize=(10, 6))\n",
        "    plt.plot(epochs, Train_Losses, label='Training Loss')\n",
        "    plt.plot(epochs, Validation_Losses, label='Validation Loss')\n",
        "    plt.title('Training and Validation Losses Over Epochs')\n",
        "    plt.xlabel('Epochs')\n",
        "    plt.ylabel('Mean Squared Error Loss')\n",
        "    plt.legend()\n",
        "    plt.show()\n",
        "## Load Data\n",
        "\n",
        "data=pd.read_csv('/content/drive/MyDrive/ce889_dataCollection.csv')\n",
        "\n",
        "NN=Neural_Network(Input_layer_size = 2 , Hidden_layer_size =11, Output_layer_size = 2 )\n",
        "\n",
        "Normalized_Data = NN.Normalize_data(data.values)\n",
        "\n",
        "# Split data into train, validation, and test sets\n",
        "Normalized_Data_Shuffled = shuffle(Normalized_Data, random_state=42)\n",
        "Train_data, Validation_data, Test_data = NN.data_split(Normalized_Data_Shuffled)\n",
        "\n",
        "# Separate features and label\n",
        "x_Train, y_Train = Train_data[:, :2], Train_data[:, :2]\n",
        "x_Validation, y_Validation = Validation_data[:, :2], Validation_data[:, :2]\n",
        "x_Test, y_Test = Test_data[:, :2], Test_data[:, :2]\n",
        "\n",
        "## Train Neural Network\n",
        "Train_Losses, Validation_Losses, Test_Losses = NN.Training(x_Train, y_Train, x_Validation, y_Validation, x_Test, y_Test, learning_rate=0.1, momentum=0.8, epochs=100)\n",
        "\n",
        "prediction = NN.prediction(x_Test)\n",
        "print(\"Predicted Output (Velocity X, Velocity Y):\", prediction)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RvPASmzKnwPG",
        "outputId": "2a94e133-2f45-4f99-bc0d-fe4a0701b6a6"
      },
      "outputs": [],
      "source": [
        "NN.Weights_I_H"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pkwf_b8dn2Sc",
        "outputId": "edcbf883-9967-4a7a-b660-2a5baf2efa21"
      },
      "outputs": [],
      "source": [
        "NN.Weights_H_O"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mylb7Tban4Qn",
        "outputId": "c77b97d0-1b3c-4199-9215-7feea8204c46"
      },
      "outputs": [],
      "source": [
        "NN.Bias_H"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "q9Fq_cWqn6KM",
        "outputId": "396cff06-dbf3-4b01-eb45-4f071395696a"
      },
      "outputs": [],
      "source": [
        "NN.Bias_O"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4QIuxNHEn8Kd"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
